# MyTorch â€“ Neural Network from Scratch

This project is part of the **AI702: Deep Learning** course at MBZUAI. The objective was to build a **fully functional neural network library from scratch**, without using PyTorch or TensorFlow autograd. All computations were done using NumPy.

## ğŸ§  Features Implemented

- âœ… Linear Layers (`mytorch/nn/linear.py`)
- âœ… Activation Functions: Sigmoid, Tanh, ReLU
- âœ… Loss Functions: MSE Loss, Cross Entropy Loss
- âœ… Stochastic Gradient Descent Optimizer (with optional Momentum)
- âœ… Batch Normalization
- âœ… Fully Connected MLP models with 0, 1, and 4 hidden layers

## ğŸ“ Folder Structure

```
mytorch-mlp/
â”œâ”€â”€ mytorch/
â”‚   â”œâ”€â”€ nn/                  # Linear, activation, loss, batchnorm
â”‚   â””â”€â”€ optim/               # SGD optimizer
â”œâ”€â”€ models/                  # MLP0, MLP1, MLP4 model definitions
â”œâ”€â”€ autograder.py            # Local autograder to test implementations
â”œâ”€â”€ autograder_flags.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```


## ğŸ“¦ Installation

### 1. Clone the repository
```bash
git clone https://github.com/h-abid97/rag-chatbot.git
cd rag-chatbot
```

### 2. Install dependencies
```bash
pip install -r requirements.txt
```


### 3. Running the Local Autograder
Test your components locally by setting the flags in autograder_flags.py and then running:
```bash
python autograder.py
```

## ğŸ“Œ Notes
- This project is educational and inspired by PyTorch-style architecture.
- No external deep learning frameworks were used.
- BatchNorm and backpropagation were implemented from scratch.